{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Makine Öğrenmesi\n",
    "\n",
    "    İlk derste tensorflowdaki veri yapılarını ve nasıl kullanıldıklarını inceledik. Şimdi de bu veri yapılarını \n",
    "    kullanarak basit bir yapay sinir ağı oluşturalım.\n",
    "    \n",
    "    Aşağıdaki resimleri inceleyerek gerçek bir sinir hücresinin matematiksel olarak nasıl ifade edildiği hakkında \n",
    "    fikir sahibi olabilirsiniz.\n",
    "![](ole.gif)\n",
    "![](img18.gif) \n",
    "\n",
    "$$Sinyal = AktivasyonFonksiyonu({AgirlikMatrisi * Girdi + SapmaDegeri})$$\n",
    "\n",
    "        Agirlik matrisleri (weights) yapay sinir aglarindaki hafizayi temsil ediyor.\n",
    "        Yapay Sinir Aginda bu matrisleri, gelen veriler ve sonrasinda kullandigimiz optimizasyon algoritmasi\n",
    "        ile sekillendiriyoruz. Yapay sinir agi boyle ogreniyor.\n",
    "\n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      Agirlik matrisini sadece girdilerle sekillendirerek aktivasyon fonksiyonunu esnek bir sekilde kullanmayiz. \n",
    "      Aktivasyon fonksiyonlarından biri olan sigmoid fonksiyonunun biassız ve bias eklenmiş grafiklerini\n",
    "      inceleyelim.\n",
    "$$ sigmoid: y = 1/(1+e^{-x}) $$\n",
    "\n",
    "![](neuron_1.gif)![](sigmoid_1.png) ![](neuron_2.gif) ![](sigmoid_2.png)\n",
    "    \n",
    "    Aktivasyon fonksiyonu yapay sinir hucresinin urettigi sinyalin gucune gore hucrenin aktif olup olmayacagini\n",
    "    veya ne olcude aktif olacagini belirler.\n",
    "    sigmoid, tanh, step bu fonksiyonlara örnek verilebilir \n",
    "\n",
    "    Yapay zekanın ogrenmesini saglamak icin tahmin edilen veriler ile dogru verileri kıyaslayıp,\n",
    "    aralarındaki kaybi(hatayi) hesaplayip azaltmamiz gerekiyor\n",
    "    \n",
    "    Bu hatayi hesaplamak icin;\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y_real))    \n",
    "    islemini kullanıyoruz. Burada oncelikle iki deger arasindaki farkı hesapladık. \n",
    "    Ardindan belirginlestirmek icin farkin karesini aldik. Son olarak da reduce_mean() \n",
    "    fonksiyonu ile kaybin ortalama degerini olctuk. Bu fonksiyon parametre olarak \n",
    "    input_tensor (giris matrisini) aliyor. Ek olarak hangi boyuta gore ortalama alinacagi\n",
    "    belirlenebiliyor.\n",
    "\n",
    "    Bu hatayi optimizasyon algoritmalari ile azaltabiliriz. Tensorflow bize hazir kullanabilecegimiz \n",
    "    bazi optimizasyon algoritmalari sunuyor. Biz bunlar arasindan GradientDescentOptimizer'i kullanacagiz.\n",
    "    \n",
    "    Aşağıdaki animasyonda bazı optimizasyon algoritmalarını minimum hatayı bulma hızları kıyaslanıyor.\n",
    "![](Long-Valley-Training-Algorithms.gif)\n",
    "\n",
    "    Bu algoritma N boyutlu hata matrisinde minumum noktayi bulmayi hedefliyor.\n",
    "    Bu algoritmayi;\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "    seklinde kullanabiliriz. Girdigimiz 0.5 degeri ogrenme oranini temsil ediyor. \n",
    "    Bu degeri cok yuksek yaparsak minumum noktayi es gecebiliriz, cok kucuk yaparsak da\n",
    "    minimum noktaya ulasmamiz cok uzun surebilir.\n",
    "\n",
    "## Şimdi bu temel bilgilerle bir yapay sinir ağı tasarlayalım\n",
    "\n",
    "    Oluşturulabilecek en basit yapay sinşr ağı 3 katmandan oluşuyor. \n",
    "    Bunlar giriş, gizli katman ve çıkış katmanı.\n",
    "    Sadece veri giriş katmanı ve çıkış katmanlarıyla oluştursaydık oluşturacağımız modelin bir şeyleri öğrenme\n",
    "    imkanı olmazdı. Verilen girdiye göre bir çıktı üreten sıradan bir program tasarlamış olurduk.\n",
    "#### Veri Giriş Katmanı\n",
    "    Bu katman adından da anlaşılacağı üzere modele verilerin girişini sağlıyor. Bu verilerin matris olarak ifade\n",
    "    edilmesi gerekiyor. Ses, resim veya kelimeler bir şekilde matrislere çevrilip bu katman aracılığıyla modele\n",
    "    iletilir.\n",
    "#### Gizli Katman\n",
    "    Bu katmana gelen veriler yukarda anlatılan işlemlerden geçer. Ağırlık matrisiyle çarpılır, bias eklenir \n",
    "    aktivasyon fonksiyonundan geçer ve elde edilen sonuç varsa diğer gizli katmanlara yoksa doğrudan çıkış katmanına\n",
    "    iletilir.\n",
    "#### Çıkış Katmanı\n",
    "    Bu katmanda veriler modelden alınır. Buradan alınan çıktı amaca göre olasılık değerlerine dönüştürülebilir.\n",
    "    \n",
    "    Aşağıdaki animasyon sayesinde temel sinir ağının işleyişi hakkında fikir sahibi olabilirsiniz.\n",
    "![](ann_3.gif)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Şimdi de bu modeli kodlayalım\n",
    "    Giris verileri icin (X) -1 ile 1 arasinda rastgele sayilardan olusan 3x3 bir matris olusturduk.\n",
    "    Gercek verileri temsil etmek icinde giris verilerinde biraz oynama yaptik. Bu ornekteki amacimiz \n",
    "    giris değerleri(X) ile yapay sinir agini egiterek ve test verileriyle kıyaslatarak her adimda gercek \n",
    "    degerlere (y_real) biraz daha yaklasmak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39979604  0.36067289  0.34455836]\n",
      " [ 0.33221036  0.38466418  0.33547926]\n",
      " [ 0.33324617  0.32537547  0.3506926 ]]\n"
     ]
    }
   ],
   "source": [
    "# Rastgele veriler oluşturduk\n",
    "X = np.random.rand(3, 3).astype(np.float32)\n",
    "y_real = X * 0.1 + 0.3\n",
    "print(y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0)) # 1D array,  [-0.322323, 0.612312, ... , 0.712342]\n",
    "biases = tf.Variable(tf.zeros([1])) # [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = weights * X + biases # giris verilerine gore bir tahmin olustur (sinir hucresi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y_pred - y_real)) # kaybi hesapla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.5) # ogrenme orani < 1\n",
    "train = optimizer.minimize(loss) # modeli optimize et"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Eğer Variable() tanımlamışsak tensorflowun  bu değişkenleri tanıması için \n",
    "    tf.global_variables_initializer() fonksiyonunu çalıştırmamız gerekiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() # onemli, degiskenleri tanimla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Eğitimin her aşamasında modelin tahmit ettiği değerlerin asıl değerlere yaklaşmasını umuyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.44382203] [ 0.15499692]\n",
      "200 [ 0.10003161] [ 0.29998285]\n",
      "400 [ 0.10000016] [ 0.29999992]\n",
      "600 [ 0.10000016] [ 0.29999992]\n",
      "800 [ 0.10000016] [ 0.29999992]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "sess = tf.Session() # pointer\n",
    "sess.run(init) # degiskenler tanimlandi\n",
    "for step in range(1000):\n",
    "    sess.run(train) # Modeli 1 kez egittik. train -> optimizer -> loss -> (y_pred, y_real) -> Wx_plus_B -> W, X, B\n",
    "\n",
    "    if step % 200 is 0: # 200 adimda bir\n",
    "        # agirlik matrisini ve sapma degerini yazdirdik. (Amacimiz  y_pred'in, y_real'e yaklasmasi)\n",
    "        print(step, sess.run(weights), sess.run(biases)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3997961   0.36067289  0.34455833]\n",
      " [ 0.33221033  0.38466424  0.33547923]\n",
      " [ 0.33324611  0.32537541  0.35069257]]\n",
      "\n",
      "[[ 0.39979604  0.36067289  0.34455836]\n",
      " [ 0.33221036  0.38466418  0.33547926]\n",
      " [ 0.33324617  0.32537547  0.3506926 ]]\n"
     ]
    }
   ],
   "source": [
    "predicted = sess.run(y_pred) # Modelden verileri almak için run metodunu bir değişkene atıyoruz\n",
    "print(predicted, end=\"\\n\\n\")\n",
    "print(y_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Çıkarımlar\n",
    "\n",
    "    Görüldüğü üzere bir yapay sinir ağı oluşturup eğitmek teorik olarak aşırı karmaşık gözükse de \n",
    "    matematiksel ifadesi oldukça anlaşılır ve kodlaması çok da zor değil.\n",
    "    \n",
    "    Prosedürü adım adım özetlemek gerekirse:\n",
    "### 1- Veri Hazırlama\n",
    "        Yukardaki örnekte kendimiz rastgele test ve eğitim veriler oluşturduk ama gerçekte bu veriler\n",
    "        özenle hazırlanmış datasetlerden oluşuyor. Veri boyutunun çok büyük olduğu durumlarda big data çözümleriyle\n",
    "        analiz edilip datasetlerin hazırlanması gerekiyor.\n",
    "        \n",
    "### 2- Yapay Sinir Ağı Modelini Oluşturma\n",
    "        Amacımıza göre modelimizi ve ağ katmanlarını oluşturuyoruz. Her bir ağ katmanını tek tek\n",
    "        oluşturmaktansatensorflowu sarmalayıcı bir kütüphane olan tflearni kullanabiliriz. Tabiki doğrudan tensorflow\n",
    "        ile çalışmak bu işin temelini öğrenmemiz açısından daha sağlıklı olacaktır.\n",
    "        \n",
    "### 3- Modeli Eğitme\n",
    "        Verilerin büyüklüğüne, sinir ağının kompleksliğine ve sistemin işlem kapasitesine göre modelin eğitim süresi\n",
    "        saniyelerden aylara kadar değişebiliyor. Bu süreci hızlandırmak için ekran kartları veya paralel sistemler\n",
    "        kullanılabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
