{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notebook'u yayınladığım site bu işi bilim adına gönüllü olarak yapıyor. \n",
    "\n",
    "# Henüz hesap ve dosya erişim sistemini oluşturmadılar.\n",
    "\n",
    "# Diğerlerininde faydalanmasını sağlamak için lütfen notebook'u \n",
    "\n",
    "# Sadece Çalıştırın Değiştirmeyin\n",
    "\n",
    "# Düzenlemek İçin File -> Make a Copy yolunu kullanarak notebook'u klonlayabilirsiniz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Tensorflow C++ da yazıdı bu yüzden pythondaki verileri doğrudan kütüphanede kullanamıyoruz.\n",
    "    Verileri kullanabilmek için tensorflowun bize sunduğu veri yapılarını kullanmamız gerekiyor.\n",
    "\n",
    "    - Variable() değişkenleri\n",
    "    - constant() sabit değerleri\n",
    "    - placeholder() geçici olarak değer tutan veri yapılarını temsil ediyor\n",
    "\n",
    "    Bu veri yapılarında çeşitli boyutlardaki matrisleri tutabiliriz. \n",
    "    Veri türü olarak python listelerini, numpy dizilerini veya tensorflow dizilerini kullanabiliriz.\n",
    "    Ayrıca istersek tanımladığımız veri yapılarını adlandırabiliriz. Tensorflow öntanımlı olarak 32 bit float\n",
    "    veriler ile işlemler yapıyor.\n",
    "\n",
    "    Kullanımı:\n",
    "        v = tf.Variable(matris, name=\"isim\")\n",
    "        p = tf.placeholder(matris, name=\"isim\")\n",
    "        c = tf.constant(matris, name=\"isim\")\n",
    "\n",
    "\n",
    "\n",
    "    Yapay Sinir Hucresi:\n",
    "$$Sinyal = AktivasyonFonksiyonu({AgirlikMatrisi * Girdi + SapmaDegeri})$$\n",
    "\n",
    "    Agirlik matrisleri (weights) yapay sinir aglarindaki hafizayi temsil ediyor.\n",
    "    Yapay Sinir Aginda bu matrisleri, gelen veriler ve sonrasinda kullandigimiz optimizasyon algoritmasi\n",
    "    ile sekillendiriyoruz. Yapay sinir agi boyle ogreniyor.\n",
    "\n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      Agirlik matrisini sadece girdilerle sekillendirerek aktivasyon fonksiyonunu esnek bir sekilde kullanmayiz. \n",
    "      Aktivasyon fonksiyonlarından biri olan sigmoid fonksiyonunun biassız ve bias eklenmiş grafiklerini\n",
    "      inceleyelim.\n",
    "$$ sigmoid: y = 1/(1+e^{-x}) $$\n",
    "\n",
    "![](neuron_1.gif)![](sigmoid_1.png) ![](neuron_2.gif) ![](sigmoid_2.png)\n",
    "    \n",
    "    Aktivasyon fonksiyonu yapay sinir hucresinin urettigi sinyalin gucune gore hucrenin aktif olup olmayacagini\n",
    "    veya ne olcude aktif olacagini belirler.\n",
    "    sigmoid, tanh, step bu fonksiyonlara örnek verilebilir \n",
    "\n",
    "    Yapay zekanın ogrenmesini saglamak icin tahmin edilen veriler ile dogru verileri kıyaslayıp,\n",
    "    aralarındaki kaybi(hatayi) hesaplayip azaltmamiz gerekiyor\n",
    "    \n",
    "    Bu hatayi hesaplamak icin;\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y_real))    \n",
    "\n",
    "    islemini kullanıyoruz. Burada oncelikle iki deger arasindaki farkı hesapladık. \n",
    "    Ardindan belirginlestirmek icin farkin karesini aldik. Son olarak da reduce_mean() \n",
    "    fonksiyonu ile kaybin ortalama degerini olctuk. Bu fonksiyon parametre olarak \n",
    "    input_tensor (giris matrisini) aliyor. Ek olarak hangi boyuta gore ortalama alinacagi\n",
    "    belirlenebiliyor.\n",
    "\n",
    "    Bu hatayi optimizasyon algoritmalari ile azaltabiliriz. Tensorflow bize hazir kullanabilecegimiz \n",
    "    bazi optimizasyon algoritmalari sunuyor. Biz bunlar arasindan GradientDescentOptimizer'i kullanacagiz.\n",
    "![](gdo.png)\n",
    "\n",
    "    Bu algoritma N boyutlu hata matrisinde minumum noktayi bulmayi hedefliyor.\n",
    "    Bu algoritmayi;\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "    seklinde kullanabiliriz. Girdigimiz 0.5 degeri ogrenme oranini temsil ediyor. \n",
    "    Bu degeri cok yuksek yaparsak minumum noktayi es gecebiliriz, cok kucuk yaparsak da\n",
    "    minimum noktaya ulasmamiz cok uzun surebilir. \n",
    "    \n",
    "    Tensorflow \"lazy\" calisiyor yani tanimladigimiz degiskenler, fonsiyonlar, optimizasyon algoritmalari \n",
    "    tanimladigimiz anda calistirilmiyor. Bu islemleri aktif etmek icin tensorflow oturumu (Session) kullanmamiz\n",
    "    gerekiyor. Session'u bir isaretci olarak dusunulebilir. Tanimladigimiz yapay sinir agi modelinin istedigimiz\n",
    "    adimini Session ile calistirip degerini alabiliriz.\n",
    "\n",
    "    Asagida en basit haliyle bir yapay sinir agi olusturduk. Giris verileri icin (X) -1 ile 1 arasinda rastgele sayilardan olusan 3x3 bir matris olusturduk.\n",
    "    Gercek verileri temsil etmek icinde giris verilerinde biraz oynama yaptik. Bu ornekteki amacimiz giris matrisini(X) ile yapay sinir agini egiterek her adimda\n",
    "    gercek degerlere(y_real) biraz daha yaklasmak. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.30501819  0.37679443  0.39487767]\n",
      " [ 0.32714039  0.36743698  0.32770458]\n",
      " [ 0.38853815  0.38401783  0.38375115]]\n"
     ]
    }
   ],
   "source": [
    "# Rastgele veriler oluşturduk\n",
    "X = np.random.rand(3, 3).astype(np.float32)\n",
    "y_real = X * 0.1 + 0.3\n",
    "print(y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0)) # 1D array,  [-0.322323, 0.612312, ... , 0.712342]\n",
    "biases = tf.Variable(tf.zeros([1])) # [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = weights * X + biases # giris verilerine gore bir tahmin olustur (sinir hucresi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y_pred - y_real)) # kaybi hesapla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.5) # learning_rate < 1\n",
    "train = optimizer.minimize(loss) # modeli optimize et"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Eğer Variable() tanımlamışsak tensorflowun  bu değişkenleri tanıması için \n",
    "    tf.global_variables_initializer() fonksiyonunu çalıştırmamız gerekiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() # important, degiskenleri tanimla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Eğitimin her aşamasında modelin tahmit ettiği değerlerin asıl değerlere yaklaşmasını umuyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.02979365] [ 0.60090828]\n",
      "200 [ 0.09999987] [ 0.3000001]\n",
      "400 [ 0.09999988] [ 0.3000001]\n",
      "600 [ 0.09999988] [ 0.3000001]\n",
      "800 [ 0.09999988] [ 0.3000001]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "sess = tf.Session() # pointer\n",
    "sess.run(init) # degiskenler tanimlandi\n",
    "for step in range(1000):\n",
    "    sess.run(train) # Modeli 1 kez egittik. train -> optimizer -> loss -> (y_pred, y_real) -> Wx_plus_B -> W, X, B\n",
    "\n",
    "    if step % 200 is 0: # 200 adimda bir\n",
    "        # agirlik matrisini ve sapma degerini yazdirdik. (Amacimiz  y_pred'in, y_real'e yaklasmasi)\n",
    "        print(step, sess.run(weights), sess.run(biases)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.30501825  0.3767944   0.39487764]\n",
      " [ 0.32714045  0.36743701  0.32770464]\n",
      " [ 0.38853812  0.38401783  0.38375115]]\n",
      "\n",
      "[[ 0.30501819  0.37679443  0.39487767]\n",
      " [ 0.32714039  0.36743698  0.32770458]\n",
      " [ 0.38853815  0.38401783  0.38375115]]\n"
     ]
    }
   ],
   "source": [
    "prediction = sess.run(y_pred) # Modelden verileri almak için run metodunu bir değişkene atıyoruz\n",
    "print(prediction, end=\"\\n\\n\")\n",
    "print(y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ders_2.py\n",
    "\"\"\"\n",
    "Data types\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53 26]\n",
      " [30 15]]\n"
     ]
    }
   ],
   "source": [
    "matrix_1 = tf.constant([[3, 3, 5], [1, 2, 3]])\n",
    "matrix_2 = tf.constant([[3, 2], [3, 5], [7, 1]])\n",
    "\n",
    "result = tf.matmul(matrix_1, matrix_2)\n",
    "\n",
    "sess = tf.Session()\n",
    "sonuc = sess.run(result)\n",
    "print(sonuc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "state = tf.Variable(0, name=\"counter\") # baslangic degeri, isim\n",
    "print(state.name)\n",
    "\n",
    "one = tf.constant(1)\n",
    "\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "init = tf.global_variables_initializer() # degisken varsa calistirilmali\n",
    "\n",
    "sess.run(init)\n",
    "for _ in range(5):\n",
    "    sess.run(update)\n",
    "    print(sess.run(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_1 = tf.placeholder(tf.float32) # type, shape\n",
    "input_2 = tf.placeholder(tf.float32)\n",
    "\n",
    "output = tf.multiply(input_1, input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 28.]\n"
     ]
    }
   ],
   "source": [
    "for step in range(1000):\n",
    "    res = sess.run(output, feed_dict={input_1:[4.], input_2:[7.]})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ders_3.py\n",
    "\"\"\"\n",
    "Convolutional layers\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def add_layer(inputs, in_size, out_size, activation_func=None):\n",
    "    \"\"\"\n",
    "        Creates hidden layers\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.random_normal([in_size, out_size])) # initialize all variables as 0.1\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    wx_plus_b = tf.matmul(inputs, weights) + biases\n",
    "\n",
    "    if activation_func:\n",
    "        return activation_func(wx_plus_b)\n",
    "    return wx_plus_b\n",
    "\n",
    "# Create some data\n",
    "X = np.linspace(-1, 1, 300)[:, np.newaxis] # add 1D to sample\n",
    "noise = np.random.normal(0, 0.05, X.shape).astype(np.float32) # add some noise so it's looks real\n",
    "# y = x^2\n",
    "Y = np.square(X) - 0.5 #+ noise\n",
    "\n",
    "# plot the data\n",
    "#plt.scatter(X, Y)\n",
    "#plt.show()\n",
    "\n",
    "# Define inputs (placeholders)\n",
    "xs = tf.placeholder(tf.float32, [None, 1]) # n samples, n futures\n",
    "ys = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Add hidden layer\n",
    "l1 = add_layer(xs, 1, 10, activation_func=tf.nn.relu) # data, future size, hidden neuron, actv_fnc\n",
    "\n",
    "# Add output layer\n",
    "prediction = add_layer(l1, 10, 1, activation_func=None) # data, future size, expectation size, actv_fnc\n",
    "\n",
    "# Calculate errors\n",
    "# calculate the square errors (difference between real and predicted values) and sum all of them\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))\n",
    "\n",
    "# Train\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(X, Y)\n",
    "plt.ion()\n",
    "plt.show()\n",
    "for i in range(1000):\n",
    "    # egitim sart\n",
    "    sess.run(train_step, feed_dict = {xs:X, ys:Y})\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        # loss is based on ys and prediction and prediction is based on xs \n",
    "        #print(sess.run(loss, feed_dict = {xs:X, ys:Y}))\n",
    "        \n",
    "        try:\n",
    "            ax.lines.remove(lines[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        prediction_value = sess.run(prediction, feed_dict={xs:X})    \n",
    "        # plot\n",
    "        lines = ax.plot(X, prediction_value, 'r', lw=5)\n",
    "        plt.pause(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
